# Integrated workshop - visual computing

## Workshop Summary

This repository documents the work completed for the workshop, showing interactive visual experiences that integrate 3D modeling, PBR materials, custom shaders, dynamic textures, multimodal sensing (voice, gestures, EEG), and camera or environment control. Each section explores a component of the graphics and sensory pipeline, uniting visual perception, light physics, procedural geometry, and human-computer interaction.

## Exercises

This section details the exercises that have been completed so far.

---

### 7. Webcam Gesture Control

- **Brief Explanation:** This project implemented a real-time gesture control system using Python and MediaPipe. The initial version focused on core gesture recognition: counting raised fingers and detecting a "pinch" by measuring the distance between the thumb and index finger. These gestures were mapped to control the color, size, and position of a circle. The project was then extended into a bonus minigame where the user must "pop" a randomly appearing target by moving their hand and using the pinch gesture.
- **Key Results (GIFs):**

|         Gesture Controlled Object          |          "Pop the Target" Minigame          |
| :----------------------------------------: | :-----------------------------------------: |
| ![Interactive gif](./gifs/07/gestures.gif) | ![Interactive gif](./gifs/07/mini-game.gif) |

- **Link to Code:**
  - [View Gesture Control Code](./exercises/07_webcam_gestures_mediapipe_hands/)
- **Personal Comments:**
  - **Learning:** This was an excellent introduction to the power of pre-trained models like MediaPipe. The primary learning was in translating raw landmark coordinates into robust, meaningful gestures. Implementing the minigame logic was a great exercise in managing application state and event detection.
  - **Challenges:** The main technical challenge was devising a reliable logic for counting fingers, especially the thumb. For the minigame, tuning the gesture thresholds for a responsive but not overly sensitive interaction was key.

---

### 8. Voice Recognition and Command Control

- **Brief Explanation:**  
  This experiment integrates **Python voice recognition** with a **Processing visualization**. Spoken commands like â€œforwardâ€, â€œleftâ€, or â€œstopâ€ are recognized through the microphone and transmitted via **OSC** to control on-screen elements in real time. The system also provides **spoken feedback** using text-to-speech, creating a complete voice-driven interaction loop.

- **Core Technologies:**  
  Python (`SpeechRecognition`, `pyttsx3`, `python-osc`, `pyaudio`) and Processing (`oscP5`).

- **Key Results (GIFs):**

  ![ðŸŽ¥ gif](./gifs/08/voice_control.gif)

- **ðŸ“½ï¸ Demo Video**

  - [ðŸŽ¥ Watch Demo Video](./assets/08/voice_control.mp4)

- **Link to Code:**

  - [View Code](./exercises/08_voice_recognition_and_command_control/)

- **Personal Comments:**
  - **Learning:** Exploring OSC communication between Python and Processing helped bridge audio input and graphical output effectively.
  - **Challenges:** Setting up PyAudio and achieving stable voice recognition accuracy required careful tuning of microphone sensitivity and noise thresholds.

---

### 9. Multimodal Interfaces: Voice + Hand Gestures (OSC â†’ Processing)

- **Brief Explanation:**  
  Real-time control of a Processing sketch using **voice** and **hand gestures** via Python. Supports solo actions (voice _or_ gesture) and **combo** (voice+gesture within a short time window). The combo (â€œhand upâ€ + â€œadelanteâ€) triggers a higher-priority action.

- **Core Technologies:**  
  Python (`OpenCV`, `MediaPipe`, `SpeechRecognition`, `PyAudio`, `pyttsx3` optional, `python-osc`) + Processing (`oscP5`).

- **Key Results (GIF / Video):**  
  ![Multimodal demo](./gifs/09/multimodal.gif)  
  **Demo video:** [â–¶ï¸ Watch](./assets/09/multimodal.mp4)

- **OSC Mapping (Python â†’ Processing):**

  | Event Source      | OSC Address        | Processing Action                    | Color (RGB)        |
  | ----------------- | ------------------ | ------------------------------------ | ------------------ |
  | Gesture only      | `/saludo`          | `currentColor = color(255, 255, 0);` | Yellow (255,255,0) |
  | Voice: â€œadelanteâ€ | `/adelante`        | `currentColor = color(0, 100, 255);` | Blue (0,100,255)   |
  | Voice: â€œdetenerâ€  | `/detener`         | `currentColor = color(0);`           | Black (0,0,0)      |
  | **Combo (V+G)**   | `/adelante_rapido` | `currentColor = color(0, 255, 0);`   | Green (0,255,0)    |

- **Link to Code:**

  - [View Multimodal Code](./exercises/09_multimodal_interfaces_voice_gestures/)
  - Main script: `multimodal_control.py`

- **Personal Comments:**
  - **Learning:** Built a thread-safe event bus with timestamps, debounced gesture detection, and a **combo hold** to fuse modalities reliably.
  - **Challenges:** Tuning gesture threshold/frame window for responsiveness vs. stability; handling mic noise; aligning OSC routes with the Processing sketch.

---
## Exercise 10 â€” BCI Simulation (Synthetic EEG and Control)

### Objective
Simulate a Brain-Computer Interface (BCI) using synthetic EEG signals to classify mental states and trigger visual actions in real-time.

### General Description
1. Generation of **synthetic EEG signals** with Alpha (8-12 Hz) and Beta (13-30 Hz) bands.
2. Implementation of **Butterworth bandpass filters** using `scipy.signal`.
3. Calculation of **band energies** and classification based on thresholds.
4. **Interactive controls** to simulate relaxed, active, and normal mental states.
5. Real-time visualization with **PyGame** showing state changes through color, size, and metrics display.

### Evidence
- **Animated GIF:** showing mental state transitions and interactive controls.
  
  ![bci_simulation](./gifs/10/bci_simulation.gif)


**Link to the code:**
> [Python Code](./exercises/10_bci_simulation_synthetic_eeg_control/python/simulacion_BCI.py)

### Personal Comments
- **Learning:** I understood how EEG signals are processed and filtered to extract meaningful brain activity patterns.
- **Challenge:** Implementing the bandpass filters correctly and tuning the classification thresholds for realistic state transitions.
- **Insight:** The simulation effectively demonstrates how BCIs work by mapping brain signals to visual outputs.

### Prompts Used
- "CÃ³mo implementar filtros pasa banda con scipy.signal para seÃ±ales EEG?"
- "CÃ³mo generar seÃ±ales EEG sintÃ©ticas con componentes Alpha y Beta?"
- "CÃ³mo crear una interfaz interactiva con PyGame para visualizaciÃ³n en tiempo real?"

---
## Exercise 11 â€” Projective Spaces and Projection Matrices

### Objective
Explore projective geometry concepts through the implementation of orthographic and perspective projection matrices, visualizing 3D objects from different camera viewpoints.

### General Description
1. Implementation of **homogeneous coordinates** for 3D point representation.
2. Creation of **orthographic and perspective projection matrices** from scratch.
3. Development of **view matrices** (lookAt) for camera positioning.
4. Visualization of a 3D cube from **4 different cameras** (Frontal, Superior, Lateral, Isometric).
5. **Interactive selector** with `ipywidgets` to switch between cameras and projection types.
6. **Depth visualization** using color gradients (Z-buffer simulation).

### Evidence
- **Animated GIF:** showing camera switching and projection type changes.
  
  ![conmutacion_camaras](./gifs/11/conmutacion_camaras.gif)


**Link to the code:**
> [Colab Notebook](./exercises/11_projective_spaces_and_projection_matrices/python/Espacios_proyectivos_matrices_proyeccion.ipynb)

### Personal Comments
- **Learning:** I gained deep understanding of how 3D graphics pipelines work, from world coordinates to screen space through matrix transformations.
- **Challenge:** Implementing the projection matrices correctly and understanding the mathematical differences between orthographic and perspective projections.
- **Insight:** The interactive selector made it clear how camera position and projection type dramatically affect the final 2D representation of 3D objects.

### Prompts Used
- "CÃ³mo implementar coordenadas homogÃ©neas en Python?"
- "CÃ³mo crear matrices de proyecciÃ³n y vista desde cero?"

---

## Folder Structure

```
2025-11-02_taller_integrado_computacion_visual/
â”œâ”€â”€ exercises/
â”‚   â”œâ”€â”€ 01_materials_light_and_color_pbr_chromatic_models/
â”‚   â”œâ”€â”€ 02_procedural_modeling_from_code/
â”‚   â”œâ”€â”€ 03_custom_shaders_and_effects/
â”‚   â”œâ”€â”€ 04_dynamic_texturing_and_particles/
â”‚   â”œâ”€â”€ 05_image_and_video_360_visualization/
â”‚   â”œâ”€â”€ 06_input_and_interaction_ui_collisions/
â”‚   â”œâ”€â”€ 07_webcam_gestures_mediapipe_hands/
â”‚   â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”‚   â”œâ”€â”€ game.py
â”‚   â”‚   â”‚   â””â”€â”€ gesture_controller.py
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ 08_voice_recognition_and_command_control/
â”‚   â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â”‚   â””â”€â”€ receptor_voz.pde
â”‚   â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”‚   â””â”€â”€ voz_control.py
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ 09_multimodal_interfaces_voice_gestures/
â”‚   â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â”‚   â””â”€â”€ multimoal_reception.pde
â”‚   â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”‚   â””â”€â”€ multimodal_control.py
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ 10_bci_simulation_synthetic_eeg_control/
â”‚   â”‚   â””â”€â”€ python/
â”‚   â”‚       â””â”€â”€ simulacion_BCI.py
â”‚   â””â”€â”€ 11_projective_spaces_and_projection_matrices/
â”‚       â””â”€â”€  python/
â”‚           â””â”€â”€ Espacios_proyectivos_matrices_proyeccion.ipynb
â”‚
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ 01/
â”‚   â”œâ”€â”€ 02/
â”‚   â”œâ”€â”€ 03/
â”‚   â”œâ”€â”€ 04/
â”‚   â”œâ”€â”€ 05/
â”‚   â”œâ”€â”€ 06/
â”‚   â”œâ”€â”€ 08/
â”‚   â”‚   â””â”€â”€ voice_control.mp4
â”‚   â”œâ”€â”€ 09/
â”‚   â”‚   â””â”€â”€ multimodal.mp4
â”‚   â”œâ”€â”€ 10/
â”‚   â””â”€â”€ 11/
â”‚
â”œâ”€â”€ gifs/
â”‚   â”œâ”€â”€ 01/
â”‚   â”œâ”€â”€ 02/
â”‚   â”œâ”€â”€ 03/
â”‚   â”œâ”€â”€ 04/
â”‚   â”œâ”€â”€ 05/
â”‚   â”œâ”€â”€ 06/
â”‚   â”œâ”€â”€ 07/
â”‚   â”‚   â”œâ”€â”€ gestures.gif
â”‚   â”‚   â””â”€â”€ mini-game.gif
â”‚   â”œâ”€â”€ 08/
â”‚   â”‚   â””â”€â”€ voice_control.gif
â”‚   â”œâ”€â”€ 09/
â”‚   â”‚   â””â”€â”€ multimodal.gif
â”‚   â”œâ”€â”€ 10/
â”‚   â”‚   â””â”€â”€ bci_simulation.gif
â”‚   â””â”€â”€ 11/
â”‚   â”‚   â””â”€â”€ commutacion_camaras.gif
â””â”€â”€ README.md

```
